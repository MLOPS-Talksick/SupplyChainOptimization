# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QmcNNm7MPLE8Mgs1vCiHidqWxz5PT0uP
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import warnings

warnings.filterwarnings('ignore')

# Load and preprocess the dataset
def load_and_preprocess_data(file_path):
    df = pd.read_csv(file_path)

    # Handle missing values
    df.fillna(df.median(numeric_only=True), inplace=True)

    # Convert Date column to datetime
    df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)

    return df

# Enhanced feature engineering for XGBoost
def create_features(data, target_col='Total Quantity', seq_length=10):
    # Create a DataFrame with the target column
    df_features = pd.DataFrame(index=data.index)

    # Add the target
    df_features[target_col] = data[target_col]

    # Add time-based features if we have a date index
    if isinstance(data.index, pd.DatetimeIndex):
        df_features['dayofweek'] = data.index.dayofweek
        df_features['month'] = data.index.month
        df_features['quarter'] = data.index.quarter
        df_features['year'] = data.index.year
        df_features['dayofyear'] = data.index.dayofyear
        # Use simpler week of year extraction for compatibility
        df_features['weekofyear'] = data.index.isocalendar().week

    # Create lag features - use a smaller number of lags if data is limited
    max_lag = min(seq_length, len(data) // 3)  # Ensure we don't create too many lags
    if max_lag < 1:
        max_lag = 1  # At least one lag

    for lag in range(1, max_lag + 1):
        df_features[f'lag_{lag}'] = data[target_col].shift(lag)

    # Create rolling window features - use smaller windows if data is limited
    max_window = min(7, len(data) // 4)
    if max_window < 2:
        max_window = 2  # At minimum use window of 2

    window_sizes = [2, min(3, max_window), min(5, max_window), min(7, max_window)]
    window_sizes = sorted(list(set(window_sizes)))  # Remove duplicates and sort

    for window in window_sizes:
        df_features[f'rolling_mean_{window}'] = data[target_col].rolling(window=window).mean()
        df_features[f'rolling_std_{window}'] = data[target_col].rolling(window=window).std().fillna(0)  # Fill NaN with 0

    # Drop NaN values
    df_features = df_features.dropna()

    # Check if we have any data left
    if len(df_features) < 1:
        # If no data left, create a minimal feature set without lags/rolling
        df_features = pd.DataFrame(index=data.index)
        df_features[target_col] = data[target_col]
        if isinstance(data.index, pd.DatetimeIndex):
            df_features['dayofweek'] = data.index.dayofweek
            df_features['month'] = data.index.month

    return df_features

# Train XGBoost model for each product with improved feature engineering
def train_xgboost_model(product_data):
    print(f"  - Data points: {len(product_data)}")

    # Ensure we have enough data
    if len(product_data) < 10:
        print("  - Not enough data, returning default values")
        # Return default values
        return {
            'rmse': np.nan, 'mape': np.nan,
            'forecast': np.array([np.nan] * 7)
        }

    product_data = product_data[['Date', 'Total Quantity']].copy()
    product_data.sort_values(by='Date', inplace=True)

    # Set date as index for time-based feature extraction
    product_data.set_index('Date', inplace=True)

    # Create enhanced features
    seq_length = min(10, len(product_data) // 3)  # Adaptive sequence length
    engineered_data = create_features(product_data, target_col='Total Quantity', seq_length=seq_length)

    # Check if we have enough data after feature engineering
    if len(engineered_data) < 5:
        print("  - Not enough data after feature engineering, returning default values")
        return {
            'rmse': np.nan, 'mape': np.nan,
            'forecast': np.array([np.nan] * 7)
        }

    # Define target and features
    y = engineered_data['Total Quantity']
    X = engineered_data.drop('Total Quantity', axis=1)

    # Check if we have any features left
    if X.shape[1] == 0:
        print("  - No features available, returning default values")
        return {
            'rmse': np.nan, 'mape': np.nan,
            'forecast': np.array([np.nan] * 7)
        }

    # Print shape for debugging
    print(f"  - Features shape: {X.shape}, Target shape: {y.shape}")

    # Normalize features if we have data
    scaler_X = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)

    # Normalize target for better prediction
    scaler_y = MinMaxScaler()
    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

    # Split dataset - use appropriate proportions based on data size
    if len(X_scaled) <= 20:
        # If very little data, use 60/40 split
        train_size = int(len(X_scaled) * 0.6)
        val_size = 0  # No validation set
    else:
        # Normal split
        train_size = int(len(X_scaled) * 0.7)
        val_size = int(len(X_scaled) * 0.15)

    X_train = X_scaled.iloc[:train_size]
    y_train = y_scaled[:train_size]

    if val_size > 0:
        X_val = X_scaled.iloc[train_size:train_size + val_size]
        y_val = y_scaled[train_size:train_size + val_size]
        X_test = X_scaled.iloc[train_size + val_size:]
        y_test = y_scaled[train_size + val_size:]
    else:
        X_test = X_scaled.iloc[train_size:]
        y_test = y_scaled[train_size:]

    # Check if we have any test data
    if len(X_test) == 0:
        print("  - No test data available, using training data for evaluation")
        X_test = X_train
        y_test = y_train

    # Adjust model complexity based on data size
    n_estimators = min(100, max(50, len(X_train) * 2))
    max_depth = min(3, max(2, len(X_train) // 10))

    # Build XGBoost model with parameters suited to data size
    model = xgb.XGBRegressor(
        n_estimators=n_estimators,
        learning_rate=0.05,
        max_depth=max_depth,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42
    )

    # Train model
    model.fit(X_train, y_train)

    # Predict test set
    y_pred = model.predict(X_test)

    # Compute Metrics
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # Transform predictions and ground truth back to original scale for interpretable metrics
    y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
    y_pred_orig = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()

    # Handle zero values in MAPE calculation
    y_test_for_mape = np.copy(y_test_orig)
    y_test_for_mape[y_test_for_mape == 0] = 1e-10  # Replace zeros with small value
    mape = np.mean(np.abs((y_test_orig - y_pred_orig) / y_test_for_mape)) * 100

    # Forecast next 7 days - simplified approach
    # Get the last available data point
    last_data = product_data.iloc[-seq_length:].copy() if seq_length <= len(product_data) else product_data.copy()
    forecast = []
    last_value = last_data['Total Quantity'].iloc[-1]

    # For small datasets, use a simple moving average with seasonality if applicable
    for _ in range(7):
        # Simple forecast based on last value and features
        pred = last_value * (1 + np.random.normal(0, 0.05))  # Add small random variation
        forecast.append(pred)
        last_value = pred

    # If we have a trained model, try to use it for forecasting
    if len(X) > 0 and len(X.columns) > 0:
        try:
            # Get the most recent features
            last_row = X.iloc[-1:].values

            if len(last_row) > 0:
                # Make a prediction for the next day
                scaled_pred = model.predict(last_row)[0]
                first_pred = float(scaler_y.inverse_transform([[scaled_pred]])[0])
                forecast[0] = first_pred  # Replace just the first prediction
        except:
            # If anything goes wrong, just use the simple forecast we already calculated
            pass

    return {'rmse': rmse, 'mape': mape, 'forecast': np.array(forecast)}

def main():
    try:
        df = load_and_preprocess_data('ML.csv')
        print(f"Loaded data with {len(df)} rows")
    except Exception as e:
        print(f"Error loading data: {e}")
        return

    products = df['Product Name'].unique()
    print(f"Found {len(products)} unique products")

    all_metrics = {'RMSE': {}, 'MAPE': {}}
    all_forecasts = {}

    for product in products:
        print(f"\nProcessing product: {product}")

        product_data = df[df['Product Name'] == product].copy()
        if len(product_data) < 5:
            print(f"  - Insufficient data for {product}. Skipping.")
            continue

        results = train_xgboost_model(product_data)

        all_metrics['RMSE'][product] = results['rmse']
        all_metrics['MAPE'][product] = results['mape']
        all_forecasts[product] = results['forecast']

        print(f"RMSE: {results['rmse']:.2f}")
        print(f"MAPE: {results['mape']:.2f}%")

        print("\nForecast for next 7 days:")
        for i, value in enumerate(results['forecast']):
            print(f"Day {i+1}: {round(float(value), 2)}")

    # Average Metrics - exclude NaN values
    valid_rmse = [v for v in all_metrics['RMSE'].values() if not np.isnan(v)]
    valid_mape = [v for v in all_metrics['MAPE'].values() if not np.isnan(v)]

    avg_rmse = sum(valid_rmse) / len(valid_rmse) if valid_rmse else np.nan
    avg_mape = sum(valid_mape) / len(valid_mape) if valid_mape else np.nan

    print("\n--- Overall Model Performance ---")
    print(f"Average RMSE: {avg_rmse:.2f}" if not np.isnan(avg_rmse) else "Average RMSE: N/A")
    print(f"Average MAPE: {avg_mape:.2f}%" if not np.isnan(avg_mape) else "Average MAPE: N/A")

if __name__ == "__main__":
    main()